{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from the New York Times corpus, we also want to try building a corpus from a variety of online news sources, to debias our work from how the Times does their reporting. We chose to work with [NewsAPI](newsapi.org), which allowed us to perform limited querying for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import dotenv\n",
    "import sys\n",
    "sys.tracebacklimit = 0 # turn off the error tracebacks\n",
    "from newspaper import Article\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "token = os.getenv('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "myanmardict = {'q':'Rohingya&', 'from':'2017-08-24&', 'language':'en', 'pageSize':100, 'apiKey': token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myanmar = requests.get('http://newsapi.org/v2/everything?', params=myanmardict)\n",
    "myanmar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "myanmarjson = json.loads(myanmar.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the stories pulled by this request are from different branches of the same news outlet (e.g. Reuters, Reuters UK, Reuters India, etc). In an effort to deduplicate our data, we removed rows with exactly matching titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "myanmardf = pd.json_normalize(myanmarjson, record_path = ['articles'])\n",
    "myanmardf = myanmardf.loc[~myanmardf.title.duplicated(),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "myanmar_urls = []\n",
    "for i in range(0, myanmardf.shape[0]):\n",
    "    myanmar_urls.append(myanmardf.iloc[i]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(myanmar_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [newspaper3k](https://newspaper.readthedocs.io/en/latest/) package, we extract the full text of each article with an `extractArticles` method. Within the method, we attempt once more to deduplicate data, checking to make sure that the text isn't already stored in the articles dict. Additionally, when building this method, we noticed that some of the URLs returned by the query were missing a / or other punctuation, and hence would raise an exception. To overcome this, we instruct the method to continue past any broken links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractArticles(lst, dic):\n",
    "    for j in lst:\n",
    "        try:\n",
    "            art = Article(j)\n",
    "            art.download()\n",
    "            art.parse()\n",
    "            text = re.sub('\\s+',' ',art.text)\n",
    "            if text not in dic.values():\n",
    "                dic[j] = text\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "myanmar_articles = {}\n",
    "extractArticles(myanmar_urls, myanmar_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "myanmar_texts = pd.DataFrame.from_dict(myanmar_articles, orient='index', columns=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a dataframe of our article texts, we use regular expressions to remove punctuation and then cast all words to lower case, for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "myanmar_texts['text']= myanmar_texts['text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "myanmar_texts['text'] = myanmar_texts['text'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we store the variable for later use in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'myanmar_texts' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store myanmar_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the `nyt_urls_df` dataframe from the `NYT.ipynb` notebook, and perform the same steps as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r nyt_urls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_urls_df.rename({0:'url'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_urls = []\n",
    "for i in range(0, nyt_urls_df.shape[0]):\n",
    "    nyt_urls.append(nyt_urls_df.iloc[i]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1059"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nyt_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_articles = {}\n",
    "extractArticles(nyt_urls, nyt_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_texts = pd.DataFrame.from_dict(nyt_articles, orient='index', columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_texts['text']= nyt_texts['text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "nyt_texts['text'] = nyt_texts['text'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'nyt_texts' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store nyt_texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
